---
title: "homework 6 (linear models)"
author: Purnima Sharma
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)
library(p8105.datasets)
library(mgcv)
library(ggplot2)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) %>% 
  filter(
    victim_race %in% c("White", "Black"),
    city_state != "Tulsa, AL") %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
```

Start with one city.

```{r}
baltimore_df =
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")
glm(resolution ~ victim_age + victim_race + victim_sex, 
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits = 3)
```

Try this across cities.

```{r}
models_results_df =
homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = 
      map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x, family = binomial())),
     results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI"))
```

Make plots.

```{r}
models_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() + 
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Problem 2

Effects of variables on child's birthweight.

Load and clean data

```{r}
baby_df = 
  read_csv("./data/birthweight.csv") %>% 
  relocate(bwt) %>% 
  mutate(
    frace = as.factor(frace),
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    mrace = as.factor(mrace)
  ) %>% 
  drop_na(bwt, blength, gaweeks, bhead, blength, babysex) 
```

Build regression model

```{r}
# scatterplot matrix and pairwise correlations
pairs(baby_df)

proposed_model = lm(bwt ~ gaweeks + wtgain + bhead * blength + ppbmi * ppwt, data = baby_df)

broom::tidy(proposed_model) 

proposed_model = lm(bwt ~ gaweeks + wtgain + ppwt + bhead * blength, data = baby_df) 

broom::tidy(proposed_model)
```

The proposed model is built based on the outcome of scatterplot matrix and pairwise correlations to check for significant covariates and any interaction terms. Due to insignificant p-value at 5% significance level of ppbmi, it is removed from the model. Gestational age in weeks, mother's weight gain during pregnancy (lbs), mother's pre-pregnancy weight (lbs), interaction between baby's head circumference (cm) and length (cm) at birth are retained as significant factors underlying baby's birthweight.

Plot of residuals against fitted values

```{r}
modelr::add_residuals(baby_df, proposed_model) %>%
add_predictions(proposed_model) %>%
ggplot(aes(x = pred, y = resid)) +
  geom_point() + 
  geom_line(aes(y = 0), color = "red")
```

The residuals seem to be evenly distributed around zero for the predicted birthweights (gm), except for some low-end outliers with over-estimated residuals. For the most part, the cluster is around 2,000gm to 4,000gm of birthweight.

Compare models

```{r}
model_one = lm(bwt ~ blength + gaweeks, data = baby_df)
broom::tidy(model_one) 

model_two = lm(bwt ~ bhead * blength * babysex, data = baby_df)
broom::tidy(model_two) 

proposed_model = lm(bwt ~ gaweeks + wtgain + ppwt + bhead * blength, data = baby_df)

# Cross-validation, 100 repetitions 
cv_df =
  crossv_mc(baby_df, 100) %>%   
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

#fit models and get prediction errors 
cv_df = 
cv_df %>%    
  mutate(
    model_one = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = .x)), 
    model_two = map(.x = train, ~lm(bwt ~ bhead * blength * babysex, data = .x)),
    proposed_model = map(.x = train, ~lm(bwt ~ gaweeks + wtgain + ppwt + bhead * blength, data = .x))
  ) %>% 
  mutate(           
    rmse_model_one = map2_dbl(.x = model_one, .y = test, ~rmse(model = .x, data = .y)),
    rmse_model_two = map2_dbl(.x = model_two, .y = test, ~rmse(model = .x, data = .y)),
    rmse_proposed = map2_dbl(.x = proposed_model, .y = test, ~rmse(model = .x, data = .y))
  )
```

Plot to compare prediction errors

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>%    
  pivot_longer(
    everything(),   
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"   
  ) %>%         
  ggplot(aes(x = model, y = rmse)) +
  geom_violin() 
```

The results show that the standard deviation of the residuals, given by Root Mean Square Error (rmse) was the lowest in the proposed model, even if just slightly lower than the given model 2. Clearly, there was a great improvement in model-building from model 1 to model 2 and proposed model.   

## Problem 3

Analysis of distribution of two measures from a simple linear model using Bootstrap.

Load data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Build a simple linear regression model, set-up "measures" calculations 

```{r}
fit = lm(tmax ~ tmin, data = weather_df)

#get estimates (beta-0 and beta-1)

estimates_df = 
broom::tidy(fit) %>% 
  select(term, "estimate") %>% 
  pivot_wider(
    names_from = "term",
  values_from = "estimate"
  ) %>% 
janitor::clean_names() %>% 
  mutate(
    b0_hat = as.numeric(intercept),
    b1_hat = as.numeric(tmin)
  ) %>%
  select(-intercept, -tmin)

# Calculate required quantity

estimates_df =
  estimates_df %>% 
  mutate(
    log_b0b1 = log(b0_hat * b1_hat)
  )
```

Draw bootstrap samples, get measures of interest
1. log(b0 * b1)

```{r}

# create bootstrap function

boot_sample = function(df) {  
  sample_frac(df, replace = TRUE)  %>%    
    arrange(tmin) 
}

# create bootstrap samples

boot_straps = 
  tibble(                     
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

# Get bootstrap results

boot_results = 
  boot_straps %>% 
  mutate(
   models = map(.x = strap_sample, ~lm ( tmax ~ tmin, data = .x)), 
   results = map(models, broom::tidy), 
  ) %>%  
  select(strap_number, results) %>%  
  unnest(results) %>%  
  select(-std.error, -statistic, -p.value) %>% 
  group_by(term) %>% 
  pivot_wider(
    names_from = "term",
  values_from = "estimate"
  ) %>% 
janitor::clean_names() %>% 
  mutate(
    b0_hat = as.numeric(intercept),
    b1_hat = as.numeric(tmin)
  ) %>%
  select(-intercept, -tmin) %>% 
  mutate(
    log_b0b1 = log(b0_hat * b1_hat)
  )
```

Distribution Plot: log(b0 * b1)

```{r}
boot_results %>% 
  ggplot(aes(x = log_b0b1))  +
  geom_density(color = 'black', fill = 'blue') 
```

The distribution of log(b0 * b1), using 5,000 bootstrap estimates, is fairly symmetrical, a good representation of a normal distribution with average value of the measure at 2.01 approximately.


Confidence interval: log(b0 * b1)

```{r}
boot_results %>% 
  summarize(
    ci_lower = quantile(log_b0b1, 0.025), 
    ci_upper = quantile(log_b0b1, 0.975)
  ) %>% 
  knitr::kable()
```

The 95% confidence interval for the quantity "log (b0 * b1)" using 5,000 bootstrap estimates is (1.96, 2.06).


2. R-squared hat

```{r}
# extract r2-hat

r2_hat_df = 
weather_df %>% 
  bootstrap(5000, id = "strap_number") %>%   
   mutate(
    models = map(.x = strap, ~lm ( tmax ~ tmin, data = .x)), 
    results = map(models, broom::glance) 
  ) %>% 
  select(strap_number, results) %>%  
  unnest(results) %>% 
 select(strap_number, "r.squared") %>% 
  janitor::clean_names()
```

Distribution Plot: r-squared hat

```{r}
r2_hat_df %>% 
  ggplot(aes(x = r_squared))  +
  geom_density(color = 'black', fill = 'purple') 
```

The distribution of the coefficient of determination produced by 5,000 bootstrap estimates is a fair representation of a normal distribution,  with slight left-skewness, suggesting that simple linear regression might not be an ideal model for the data. The average value of the measure is 0.91 approximately.


Confidence interval: r-squared hat

```{r}
r2_hat_df  %>% 
  summarize(
    ci_lower = quantile(r_squared, 0.025), 
    ci_upper = quantile(r_squared, 0.975)
  ) %>% 
  knitr::kable()
```

The 95% confidence interval for the coefficient of determination using 5,000 bootstrap estimates is (0.893, 0.927).

***
